{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550152it [00:04, 137330.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_file = './data/snli/snli_1.0/snli_1.0_train.jsonl'\n",
    "\n",
    "train = []\n",
    "label = []\n",
    "label_map = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
    "\n",
    "with open(train_file,'r') as f:\n",
    "    for item in tqdm(jsonlines.Reader(f)):\n",
    "        if item['gold_label'] not in label_map:\n",
    "            continue\n",
    "        train.append([item['sentence1'],item['sentence2']])\n",
    "        label.append(label_map[item['gold_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "import numpy as np\n",
    "#from tensorflow import set_random_seed\n",
    "from sklearn.model_selection import train_test_split # 划分训练集和测试集\n",
    "\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "    for sent in tqdm(df):\n",
    "        lemma_words = []\n",
    "        word = re.sub('[^a-zA-Z]',' ',sent[0])\n",
    "        word = word_tokenize(word.lower())\n",
    "        lemma_words.append([lemmatizer.lemmatize(i) for i in word])\n",
    "        word = re.sub('[^a-zA-Z]',' ',sent[1])\n",
    "        word = word_tokenize(word.lower())\n",
    "        lemma_words.append([lemmatizer.lemmatize(i) for i in word])\n",
    "        reviews.append(lemma_words)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [01:50<00:00, 4980.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = clean_sentences(train)\n",
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367 549367\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences),len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [00:01<00:00, 403942.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28420\n",
      "78\n",
      "划分数据集\n"
     ]
    }
   ],
   "source": [
    "target = to_categorical(label)\n",
    "nums_classes = 3\n",
    "# 拿到词典\n",
    "words_dict = set()\n",
    "len_max = 0\n",
    "for sent in tqdm(train_sentences):\n",
    "    words_dict.update(sent[0])\n",
    "    words_dict.update(sent[1])\n",
    "    if len_max < max(len(sent[0]),len(sent[1])):\n",
    "        len_max = max(len(sent[0]),len(sent[1]))\n",
    "print(len(words_dict))\n",
    "print(len_max)\n",
    "# split dataset\n",
    "print('划分数据集')\n",
    "x_train,x_val,y_train,y_val = train_test_split(train_sentences,target,test_size=0.2,stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computing',\n",
       " 'predicting',\n",
       " 'slinkys',\n",
       " 'subaru',\n",
       " 'note',\n",
       " 'booty',\n",
       " 'subsequently',\n",
       " 'ralley',\n",
       " 'dandelion',\n",
       " 'chessboard']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words_dict)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词\n",
      "sentence to sequence\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "# 对句子进行tokenizer操作\n",
    "tokenizer = Tokenizer(num_words = len(list(words_dict)))\n",
    "sentence1 = list(np.array(train_sentences)[::,0])\n",
    "sentence2 = list(np.array(train_sentences)[::,0])\n",
    "print('分词')\n",
    "tokenizer.fit_on_texts(sentence1 + sentence2)\n",
    "print('sentence to sequence')\n",
    "x_train1 = list(np.array(x_train)[::,0])\n",
    "x_train2 = list(np.array(x_train)[::,1])\n",
    "x_val1 = list(np.array(x_val)[::,0])\n",
    "x_val2 = list(np.array(x_val)[::,1])\n",
    "x_train1 = tokenizer.texts_to_sequences(x_train1)\n",
    "x_train2 = tokenizer.texts_to_sequences(x_train2)\n",
    "x_val1 = tokenizer.texts_to_sequences(x_val1)\n",
    "x_val2 = tokenizer.texts_to_sequences(x_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding to sequence \n"
     ]
    }
   ],
   "source": [
    "# padding sequence\n",
    "print('padding to sequence ')\n",
    "x_train1 = sequence.pad_sequences(x_train1,maxlen=len_max)\n",
    "x_train2 = sequence.pad_sequences(x_train2,maxlen=len_max)\n",
    "x_val1 = sequence.pad_sequences(x_val1,maxlen=len_max)\n",
    "x_val2 = sequence.pad_sequences(x_val2,maxlen=len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(439493, 78) (439493, 78) (109874, 78) (109874, 78)\n"
     ]
    }
   ],
   "source": [
    "print(x_train1.shape,x_train2.shape,x_val1.shape,x_val2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  21,   6,   2,\n",
       "         1,  23,  16,   7, 514,   1, 258,   8, 662,   5,   1, 293, 541],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置early stop\n",
    "early_stopping = EarlyStopping(min_delta = 0.001,mode = 'max',monitor = 'val_acc',patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
