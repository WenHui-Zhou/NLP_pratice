{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据集\n",
    "import texar.torch as tx\n",
    "import os\n",
    "root = './data'\n",
    "source_file = os.path.join(root,'sources.txt')\n",
    "target_file = os.path.join(root,'targets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "import torch\n",
    "import texar.torch as tx\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 4\n",
    "display = 50\n",
    "\n",
    "source_vocab_file = './data/vocab.sources.txt'\n",
    "target_vocab_file = './data/vocab.targets.txt'\n",
    "train = {\n",
    "    'batch_size':32,\n",
    "    'source_dataset':{\n",
    "        'files':source_file,\n",
    "        'vocab_file':source_vocab_file\n",
    "    },\n",
    "    'target_dataset':{\n",
    "        'files':target_file,\n",
    "        'vocab_file':target_vocab_file\n",
    "    }\n",
    "}\n",
    "num_units = 256\n",
    "embedder = {\n",
    "    'dim': num_units\n",
    "}\n",
    "decoders = {\n",
    "    'rnn_cell': {\n",
    "        'kwargs': {\n",
    "            'num_units': num_units\n",
    "        },\n",
    "    },\n",
    "    'attention': {\n",
    "        'kwargs': {\n",
    "            'num_units': num_units,\n",
    "        },\n",
    "        'attention_layer_size': num_units\n",
    "    },\n",
    "    'max_decoding_length_infer': 60,\n",
    "}\n",
    "encoders = {\n",
    "    'rnn_cell_fw': {\n",
    "        'kwargs': {\n",
    "            'num_units': num_units\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "\n",
    "train_data = tx.data.PairedTextData(hparams=train,device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)\n",
    "train_data.source_vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import texar.torch as tx\n",
    "\n",
    "\n",
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self,train_data):\n",
    "        super(Seq2SeqAttn,self).__init__()\n",
    "        self.source_vocab_size = train_data.source_vocab.size\n",
    "        self.target_vocab_size = train_data.target_vocab.size\n",
    "        \n",
    "        self.bos_token_id = train_data.target_vocab.bos_token_id\n",
    "        self.eos_token_id = train_data.target_vocab.eos_token_id\n",
    "        \n",
    "        self.source_embedder = tx.modules.WordEmbedder(vocab_size=self.source_vocab_size,hparams = embedder)\n",
    "        self.target_embedder = tx.modules.WordEmbedder(vocab_size=self.target_vocab_size,hparams = embedder)\n",
    "        self.encoder = tx.modules.BidirectionalRNNEncoder(\n",
    "            input_size = self.source_embedder.dim,\n",
    "            hparams=encoder)\n",
    "        self.decoder = tx.modules.AttentionRNNDecoder(\n",
    "            token_embedder=self.target_embedder,\n",
    "            encoder_output_size=(self.encoder.cell_fw.hidden_size +\n",
    "                                 self.encoder.cell_bw.hidden_size),\n",
    "            input_size=self.target_embedder.dim,\n",
    "            vocab_size=self.target_vocab_size,\n",
    "            hparams=decoders)\n",
    "    def forward(self,batch,mode):\n",
    "        enc_outputs,_ = self.encoder(\n",
    "            inputs = self.source_embedder(batch['source_text_ids']),\n",
    "            sequence_length = batch['source_length']\n",
    "        )\n",
    "        memory = torch.cat(enc_outputs,dim=2)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            helper_train = self.decoder.create_helper(decoding_strategy = \"train_greedy\")\n",
    "            training_outputs,_,_ = self.decoder(memory = memory,\n",
    "                                               memory_sequence_length=batch['source_length'],\n",
    "                                               helper = helper_train,\n",
    "                                               inputs = batch['target_text_ids'][:,:-1],\n",
    "                                               sequence_length = batch['target_length'] - 1)\n",
    "            mle_loss = tx.losses.sequence_sparse_softmax_cross_entropy(labels=batch['target_text_ids'][:,1:],\n",
    "                                                                      logits=training_outputs.logits,\n",
    "                                                                      sequence_length=batch['target_length'] - 1\n",
    "                                                                      )\n",
    "            return mle_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttn(\n",
       "  (source_embedder): WordEmbedder(\n",
       "    vocab_size=24, embedding_dim=256\n",
       "    (_dropout_layer): EmbeddingDropout()\n",
       "  )\n",
       "  (target_embedder): WordEmbedder(\n",
       "    vocab_size=24, embedding_dim=256\n",
       "    (_dropout_layer): EmbeddingDropout()\n",
       "  )\n",
       "  (encoder): BidirectionalRNNEncoder(\n",
       "    (_cell_fw): LSTMCell(\n",
       "      (_cell): LSTMCell(256, 256)\n",
       "    )\n",
       "    (_cell_bw): LSTMCell(\n",
       "      (_cell): LSTMCell(256, 256)\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttentionRNNDecoder(\n",
       "    (_token_embedder): WordEmbedder(\n",
       "      vocab_size=24, embedding_dim=256\n",
       "      (_dropout_layer): EmbeddingDropout()\n",
       "    )\n",
       "    (_cell): AttentionWrapper(\n",
       "      (_cell): LSTMCell(\n",
       "        (_cell): LSTMCell(512, 256)\n",
       "      )\n",
       "      (_attention_layers): ModuleList(\n",
       "        (0): Linear(in_features=768, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (_output_layer): Linear(in_features=256, out_features=24, bias=True)\n",
       "    (attention_mechanism): LuongAttention(\n",
       "      (_memory_layer): Linear(in_features=512, out_features=256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Seq2SeqAttn(train_data)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = tx.data.TrainTestDataIterator(train=train_data,test=train_data,val = train_data)\n",
    "opt = {\n",
    "    'optimizer': {\n",
    "        'type':  'Adam',\n",
    "        'kwargs': {\n",
    "            'lr': 0.001,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "train_op = tx.core.get_train_op(params=model.parameters(),hparams=opt)\n",
    "def _train_epoch():\n",
    "    data_iterator.switch_to_train_data()\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for batch in data_iterator:\n",
    "        loss = model(batch,mode = \"train\")\n",
    "        loss.backward()\n",
    "        train_op()\n",
    "        if step % 100 == 0:\n",
    "            print(\"step = {},loss={:.4f}\".format(step,loss))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0,loss=39.2355\n",
      "step = 100,loss=1.3169\n",
      "step = 200,loss=0.7944\n",
      "step = 300,loss=1.4363\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_SPDataLoaderIter' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-148eecfb9880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-f9dbc8ba8d5f>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/texar/torch/data/data/data_iterators.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             if (self._batch_size is not None and\n\u001b[1;32m    425\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     not self.dataset.hparams.allow_smaller_final_batch):\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_SPDataLoaderIter' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    _train_epoch()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
